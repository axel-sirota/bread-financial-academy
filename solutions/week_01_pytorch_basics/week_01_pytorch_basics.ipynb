{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: PyTorch Basics - Building Neural Networks\n",
    "\n",
    "**Bread Financial - AI for Data Scientists Academy**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "- Work with PyTorch tensors and perform basic tensor operations\n",
    "- Understand automatic differentiation with autograd\n",
    "- Build feedforward neural networks using `nn.Module`\n",
    "- Implement a complete training loop with proper best practices\n",
    "- Train a neural network to classify handwritten digits (MNIST dataset)\n",
    "- Evaluate model performance and visualize predictions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this notebook, you should have:\n",
    "\n",
    "- Python programming fundamentals\n",
    "- Basic NumPy and Pandas knowledge\n",
    "- Watched pre-class videos on: neural networks, backpropagation, activation functions\n",
    "\n",
    "## Session Format\n",
    "\n",
    "- **2-hour hands-on session**\n",
    "- Instructor will demo key concepts (live coding)\n",
    "- You will complete labs independently\n",
    "- Solutions shared after class\n",
    "\n",
    "---\n",
    "\n",
    "## Important: GPU Setup for Google Colab\n",
    "\n",
    "If you're running this notebook on Google Colab, **enable GPU acceleration** for faster training:\n",
    "\n",
    "1. Click on **Runtime** in the top menu\n",
    "2. Select **Change runtime type**\n",
    "3. Under **Hardware accelerator**, select **T4 GPU**\n",
    "4. Click **Save**\n",
    "\n",
    "The notebook will work fine on CPU, but GPU makes training much faster!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Environment Setup\n",
    "\n",
    "Let's start by setting up our environment and understanding what we're building today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this first in Google Colab)\n",
    "# If running locally with conda/venv, you may skip this cell\n",
    "\n",
    "!pip install torch torchvision matplotlib numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Device configuration - automatically use GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "# This ensures everyone gets the same results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"\\n Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are We Building Today?\n",
    "\n",
    "### Real-World Context: Automated Check Processing\n",
    "\n",
    "Imagine you're a data scientist at a bank. Thousands of checks arrive daily, and clerks manually type in the check amounts. This is slow, expensive, and error-prone.\n",
    "\n",
    "**Your mission**: Build an AI system that automatically reads handwritten digits on checks.\n",
    "\n",
    "Today, we'll start with the MNIST dataset - 70,000 images of handwritten digits (0-9). This is a classic dataset that simulates the digit recognition problem banks face.\n",
    "\n",
    "Let's look at what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load a sample of MNIST data to visualize\n# Don't worry about the details yet - we'll explain everything later!\nsample_data = torchvision.datasets.MNIST(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transforms.ToTensor()\n)\n\n# Display 10 sample digits\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.flat):\n    image, label = sample_data[i]\n    ax.imshow(image.squeeze(), cmap='gray')\n    ax.set_title(f'Label: {label}', fontsize=14)\n    ax.axis('off')\n\nplt.suptitle('MNIST Handwritten Digits - Examples', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nDataset size: {len(sample_data):,} training images\")\nprint(f\"Image dimensions: 28x28 pixels (grayscale)\")\nprint(f\"Number of classes: 10 (digits 0-9)\")\nprint(f\"\\n Goal: Build a neural network that achieves ~95% accuracy!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Topic 1: PyTorch Tensors & Operations\n",
    "\n",
    "## Why PyTorch?\n",
    "\n",
    "You might be wondering: \"We already know NumPy, why learn PyTorch?\"\n",
    "\n",
    "**PyTorch offers three critical advantages:**\n",
    "\n",
    "1. **GPU Acceleration**: PyTorch tensors can run on GPUs, making computations 10-100x faster\n",
    "2. **Automatic Differentiation**: PyTorch automatically computes gradients (derivatives) for us - essential for training neural networks\n",
    "3. **Deep Learning Ecosystem**: Built-in layers, optimizers, and tools specifically designed for neural networks\n",
    "\n",
    "## What is a Tensor?\n",
    "\n",
    "A **tensor** is a multi-dimensional array - just like NumPy arrays, but optimized for deep learning:\n",
    "\n",
    "- **Scalar** (0D tensor): A single number → `5`\n",
    "- **Vector** (1D tensor): Array of numbers → `[1, 2, 3, 4]`\n",
    "- **Matrix** (2D tensor): Table of numbers → `[[1, 2], [3, 4]]`\n",
    "- **3D+ tensors**: Images (height × width × channels), videos, etc.\n",
    "\n",
    "### Example: Representing an MNIST Image\n",
    "\n",
    "Each MNIST digit is a **2D tensor** of shape `(28, 28)` containing pixel intensities:\n",
    "\n",
    "```python\n",
    "# Conceptual example (actual values)\n",
    "digit_image = torch.tensor([\n",
    "[0.0, 0.0, 0.5, 0.8, 0.8, 0.5, ...], # Row 1 (28 pixels)\n",
    "[0.0, 0.2, 0.9, 1.0, 1.0, 0.9, ...], # Row 2\n",
    "... # 28 rows total\n",
    "]) # Shape: (28, 28)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Demo: Tensor Basics\n",
    "\n",
    "The instructor will demonstrate tensor creation and operations. **Pay attention to**:\n",
    "- How to create tensors\n",
    "- Basic operations (add, multiply, reshape)\n",
    "- Moving tensors between CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Creating Tensors\n",
    "\n",
    "# From Python lists\n",
    "tensor_1d = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"1D Tensor (vector):\")\n",
    "print(tensor_1d)\n",
    "print(f\"Shape: {tensor_1d.shape}\\n\")\n",
    "\n",
    "# 2D tensor (matrix)\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"2D Tensor (matrix):\")\n",
    "print(tensor_2d)\n",
    "print(f\"Shape: {tensor_2d.shape}\\n\")\n",
    "\n",
    "# Special tensors\n",
    "zeros = torch.zeros(3, 4) # 3x4 matrix of zeros\n",
    "ones = torch.ones(2, 3) # 2x3 matrix of ones\n",
    "random = torch.randn(2, 2) # 2x2 matrix with random values (normal distribution)\n",
    "\n",
    "print(\"Special tensors:\")\n",
    "print(f\"Zeros:\\n{zeros}\\n\")\n",
    "print(f\"Ones:\\n{ones}\\n\")\n",
    "print(f\"Random:\\n{random}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Tensor Operations\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# Element-wise operations (performed on corresponding elements)\n",
    "print(\"Element-wise operations:\")\n",
    "print(f\"x + y = {x + y}\") # [1+4, 2+5, 3+6] = [5, 7, 9]\n",
    "print(f\"x * y = {x * y}\") # [1*4, 2*5, 3*6] = [4, 10, 18]\n",
    "print(f\"x ** 2 = {x ** 2}\\n\") # [1^2, 2^2, 3^2] = [1, 4, 9]\n",
    "\n",
    "# Matrix operations\n",
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Matrix multiplication (dot product)\n",
    "C = torch.mm(A, B) # or A @ B\n",
    "print(\"Matrix multiplication A @ B:\")\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Reshaping Tensors\n",
    "\n",
    "# This is CRITICAL for neural networks!\n",
    "# We often need to flatten images or change tensor dimensions\n",
    "\n",
    "# Create a tensor representing an image (batch_size=1, channels=1, height=28, width=28)\n",
    "image = torch.randn(1, 1, 28, 28)\n",
    "print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "# Flatten the image to a vector (needed for feedforward networks)\n",
    "# 28 * 28 = 784 pixels\n",
    "flattened = image.view(1, 784) # or image.reshape(1, 784)\n",
    "print(f\"Flattened image shape: {flattened.shape}\")\n",
    "\n",
    "# Alternative: use -1 to infer dimension automatically\n",
    "flattened_auto = image.view(image.size(0), -1) # -1 means \"figure out this dimension\"\n",
    "print(f\"Auto-flattened shape: {flattened_auto.shape}\")\n",
    "\n",
    "print(\"\\n Key insight: .view() and .reshape() let us change tensor shape without copying data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo: Device Management (CPU vs GPU)\n\n# Create tensor on CPU (default)\ncpu_tensor = torch.tensor([1, 2, 3])\nprint(f\"CPU tensor device: {cpu_tensor.device}\")\n\n# Move tensor to GPU if available\nif device.type == 'cuda':\n    gpu_tensor = cpu_tensor.to(device)  # Copy to GPU\n    print(f\"GPU tensor device: {gpu_tensor.device}\")\n    \n    # For operations to work, tensors must be on the SAME device\n    # This would ERROR: cpu_tensor + gpu_tensor\n    \n    # Move back to CPU\n    back_to_cpu = gpu_tensor.cpu()\n    print(f\"Back to CPU: {back_to_cpu.device}\")\nelse:\n    print(\"GPU not available, staying on CPU\")\n\nprint(\"\\n Best practice: Use .to(device) to automatically handle CPU/GPU\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab 1: Tensor Operations Practice\n",
    "\n",
    "Now it's your turn! Complete the following exercises to practice working with tensors.\n",
    "\n",
    "### Exercise 1.1: Create Tensors\n",
    "\n",
    "Create the following tensors:\n",
    "1. A 1D tensor with values `[10, 20, 30, 40, 50]`\n",
    "2. A 3x3 matrix of zeros\n",
    "3. A 2x4 matrix of random values (use `torch.randn()`)\n",
    "4. A tensor representing a batch of 5 MNIST images (shape should be `(5, 1, 28, 28)`)\n",
    "\n",
    "**Hints:**\n",
    "- Use `torch.tensor()` for specific values\n",
    "- Use `torch.zeros()`, `torch.ones()`, `torch.randn()` for special tensors\n",
    "- Check shapes with `.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Lab 1.1 - Create Tensors\n\n# 1. Create 1D tensor\ntensor_1 = torch.tensor([10, 20, 30, 40, 50])\n\n# 2. Create 3x3 zeros\ntensor_2 = torch.zeros(3, 3)\n\n# 3. Create 2x4 random\ntensor_3 = torch.randn(2, 4)\n\n# 4. Create batch of MNIST-shaped tensors\ntensor_4 = torch.randn(5, 1, 28, 28)\n\n# Print shapes to verify\nprint(f\"tensor_1 shape: {tensor_1.shape}\")  # Expected: torch.Size([5])\nprint(f\"tensor_2 shape: {tensor_2.shape}\")  # Expected: torch.Size([3, 3])\nprint(f\"tensor_3 shape: {tensor_3.shape}\")  # Expected: torch.Size([2, 4])\nprint(f\"tensor_4 shape: {tensor_4.shape}\")  # Expected: torch.Size([5, 1, 28, 28])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Tensor Operations\n",
    "\n",
    "Given two tensors `a` and `b`, perform the following operations:\n",
    "\n",
    "1. Element-wise addition\n",
    "2. Element-wise multiplication\n",
    "3. Compute the mean of tensor `a`\n",
    "4. Find the maximum value in tensor `b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Lab 1.2 - Tensor Operations\n\n# Given tensors\na = torch.tensor([2.0, 4.0, 6.0, 8.0])\nb = torch.tensor([1.0, 3.0, 5.0, 7.0])\n\n# 1. Element-wise addition\naddition = a + b  # Result: [3, 7, 11, 15]\n\n# 2. Element-wise multiplication\nmultiplication = a * b  # Result: [2, 12, 30, 56]\n\n# 3. Mean of a\nmean_a = a.mean()  # or torch.mean(a) - Result: 5.0\n\n# 4. Max of b\nmax_b = b.max()  # or torch.max(b) - Result: 7.0\n\nprint(f\"a + b = {addition}\")\nprint(f\"a * b = {multiplication}\")\nprint(f\"Mean of a = {mean_a}\")\nprint(f\"Max of b = {max_b}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Reshaping for Neural Networks\n",
    "\n",
    "**Scenario**: You have a batch of 10 MNIST images (shape `(10, 1, 28, 28)`). To feed them into a feedforward neural network, you need to flatten each image into a vector of 784 pixels.\n",
    "\n",
    "**Task**: Flatten the batch so the shape becomes `(10, 784)`.\n",
    "\n",
    "**Hint**: Use `.view()` or `.reshape()` with size `(batch_size, -1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Lab 1.3 - Reshaping for Neural Networks\n\n# Create a batch of 10 random MNIST-like images\nbatch_images = torch.randn(10, 1, 28, 28)\nprint(f\"Original shape: {batch_images.shape}\")\n\n# Flatten the images using .view() or .reshape()\n# Method 1: Explicit dimensions\n# flattened_batch = batch_images.view(10, 784)\n\n# Method 2: Using -1 to auto-infer dimension (recommended)\nflattened_batch = batch_images.view(batch_images.size(0), -1)\n\nprint(f\"Flattened shape: {flattened_batch.shape}\")\nprint(f\"Expected shape: (10, 784)\")\n\n# Verify dimensions\nif flattened_batch is not None and flattened_batch.shape == (10, 784):\n    print(\"\\n Correct! You successfully flattened the batch.\")\nelse:\n    print(\"\\n Shape doesn't match. Try again!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Topic 2: Autograd & Computational Graphs\n",
    "\n",
    "## Why Do We Need Automatic Differentiation?\n",
    "\n",
    "Training a neural network requires computing **gradients** (derivatives) to know how to adjust weights:\n",
    "\n",
    "1. **Forward pass**: Input → Network → Prediction → Loss\n",
    "2. **Backward pass**: Compute gradients of loss with respect to all weights\n",
    "3. **Weight update**: Adjust weights in the direction that reduces loss\n",
    "\n",
    "Manually computing gradients for complex networks is extremely tedious and error-prone. **Autograd** does this automatically!\n",
    "\n",
    "## How Autograd Works\n",
    "\n",
    "PyTorch builds a **computational graph** tracking all operations:\n",
    "\n",
    "```\n",
    "x (requires_grad=True) → multiply by W → add b → loss\n",
    "↓ ↓ ↓\n",
    "gradients computed automatically\n",
    "```\n",
    "\n",
    "When you call `.backward()`, PyTorch:\n",
    "1. Traverses the graph backwards (chain rule)\n",
    "2. Computes gradients for all tensors with `requires_grad=True`\n",
    "3. Stores gradients in the `.grad` attribute\n",
    "\n",
    "---\n",
    "\n",
    "## Demo: Autograd in Action\n",
    "\n",
    "The instructor will demonstrate:\n",
    "- How to enable gradient tracking\n",
    "- Computing gradients with `.backward()`\n",
    "- **Why we need `zero_grad()`** (critical for training!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Simple Gradient Computation\n",
    "\n",
    "# Create a tensor and tell PyTorch to track operations on it\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "print(f\"requires_grad: {x.requires_grad}\\n\")\n",
    "\n",
    "# Perform some operations\n",
    "# Let's compute y = 3x^2 + 2x + 1\n",
    "y = 3 * x**2 + 2 * x + 1\n",
    "print(f\"y = 3x² + 2x + 1 = {y}\")\n",
    "print(f\"y requires_grad: {y.requires_grad}\\n\")\n",
    "\n",
    "# Compute gradients (dy/dx)\n",
    "# Mathematically: dy/dx = 6x + 2 = 6(2) + 2 = 14\n",
    "y.backward() # This computes gradients!\n",
    "\n",
    "print(f\"Gradient dy/dx = {x.grad}\")\n",
    "print(f\"Expected: 6x + 2 = 6(2) + 2 = 14\")\n",
    "print(\"\\n Autograd computed the derivative automatically!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Why zero_grad() is CRITICAL\n",
    "\n",
    "print(\" COMMON MISTAKE: Forgetting to zero gradients\\n\")\n",
    "\n",
    "# Create tensor\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = x ** 2 # y1 = 9, dy1/dx = 2x = 6\n",
    "y1.backward()\n",
    "print(f\"First computation: x² = {y1.item():.1f}, gradient = {x.grad.item():.1f}\")\n",
    "\n",
    "# Second computation WITHOUT zeroing gradients\n",
    "y2 = x ** 3 # y2 = 27, dy2/dx = 3x² = 27\n",
    "y2.backward() # This ADDS to existing gradient!\n",
    "print(f\"Second computation (no zero): x³ = {y2.item():.1f}, gradient = {x.grad.item():.1f}\")\n",
    "print(f\" Wrong! Gradient accumulated: 6 + 27 = 33\\n\")\n",
    "\n",
    "# Correct way: Zero gradients before each new computation\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f\"First: gradient = {x.grad.item():.1f}\")\n",
    "\n",
    "x.grad.zero_() # Zero the gradients!\n",
    "y2 = x ** 3\n",
    "y2.backward()\n",
    "print(f\"Second (with zero): gradient = {x.grad.item():.1f}\")\n",
    "print(f\" Correct! Gradient = 27\\n\")\n",
    "\n",
    "print(\" KEY TAKEAWAY: Always call zero_grad() before computing new gradients!\")\n",
    "print(\" In training loops: optimizer.zero_grad() does this for all model parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab 2: Autograd Practice\n",
    "\n",
    "### Exercise 2.1: Compute Gradients\n",
    "\n",
    "Given the function `f(x) = x³ - 2x² + 5x - 1`:\n",
    "\n",
    "1. Create a tensor `x = 4.0` with gradient tracking enabled\n",
    "2. Compute `f(x)`\n",
    "3. Compute the gradient `df/dx`\n",
    "4. Verify your answer (derivative: `f'(x) = 3x² - 4x + 5`, so `f'(4) = 3(16) - 4(4) + 5 = 48 - 16 + 5 = 37`)\n",
    "\n",
    "**Hints:**\n",
    "- Use `requires_grad=True`\n",
    "- Call `.backward()` on the result\n",
    "- Access gradient with `.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Lab 2.1 - Compute Gradients\n\n# 1. Create x with gradient tracking\nx = torch.tensor([4.0], requires_grad=True)\n\n# 2. Compute f(x) = x³ - 2x² + 5x - 1\nf = x**3 - 2*x**2 + 5*x - 1\n\n# 3. Compute gradient\nf.backward()\n\n# 4. Print results\nif x is not None and f is not None:\n    print(f\"f(4) = {f.item():.1f}\")\n    if x.grad is not None:\n        print(f\"f'(4) = {x.grad.item():.1f}\")\n    print(f\"Expected: 37.0\")\n    if x.grad is not None and abs(x.grad.item() - 37.0) < 0.01:\n        print(\"\\n Correct gradient!\")\n    else:\n        print(\"\\n Gradient doesn't match. Check your computation.\")\n\n# Explanation: \n# f(x) = x³ - 2x² + 5x - 1\n# f'(x) = 3x² - 4x + 5\n# f'(4) = 3(16) - 4(4) + 5 = 48 - 16 + 5 = 37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Understanding Gradient Accumulation\n",
    "\n",
    "**Task**: Demonstrate the gradient accumulation problem and fix it.\n",
    "\n",
    "1. Create `x = 2.0` with gradient tracking\n",
    "2. Compute `y1 = 5x²` and get the gradient (should be 20)\n",
    "3. WITHOUT zeroing, compute `y2 = 3x` and get the gradient\n",
    "4. Observe the accumulated gradient\n",
    "5. Now zero the gradient and recompute `y2` to get the correct gradient (should be 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Lab 2.2 - Understanding Gradient Accumulation\n\n# Step 1: Create x\nx = torch.tensor([2.0], requires_grad=True)\n\n# Step 2: Compute y1 and gradient\ny1 = 5 * x**2  # y1 = 5x², dy1/dx = 10x = 20\ny1.backward()\nprint(f\"After y1 = 5x²: gradient = {x.grad.item()}\")  # Should be 20\n\n# Step 3: Compute y2 WITHOUT zeroing\ny2 = 3 * x  # y2 = 3x, dy2/dx = 3\ny2.backward()  # This ADDS to existing gradient!\nprint(f\"After y2 = 3x (no zero): gradient = {x.grad.item()}\")  # Should be 23 (20+3)\nprint(\"This should be WRONG (accumulated)\\n\")\n\n# Step 4: Zero gradients\nx.grad.zero_()\n\n# Step 5: Recompute y2\ny2 = 3 * x\ny2.backward()\nprint(f\"After y2 = 3x (with zero): gradient = {x.grad.item()}\")  # Should be 3\nprint(\"This should be CORRECT (3.0)\")\n\n# Key Takeaway: Always zero gradients before computing new ones!\n# In training loops, optimizer.zero_grad() does this for all parameters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Topic 3: Building Neural Networks with nn.Module\n",
    "\n",
    "## What is nn.Module?\n",
    "\n",
    "PyTorch provides `nn.Module` as the base class for all neural networks. To create a custom neural network:\n",
    "\n",
    "1. **Subclass nn.Module**: `class MyNetwork(nn.Module):`\n",
    "2. **Define layers in `__init__`**: Create layers as attributes (they're automatically registered)\n",
    "3. **Define forward pass in `forward()`**: Specify how data flows through the network\n",
    "\n",
    "### Key Rules:\n",
    "\n",
    "- **Always call `super().__init__()`** first in your `__init__` method\n",
    "- **Define layers as attributes** in `__init__` (not in `forward`)\n",
    "- **Implement `forward()`** to define the computation\n",
    "- **DON'T create new layers in `forward`** (they won't be registered!)\n",
    "\n",
    "### For MNIST:\n",
    "\n",
    "We'll build a simple feedforward network:\n",
    "- **Input**: 784 pixels (28×28 flattened)\n",
    "- **Hidden Layer**: 128 neurons with ReLU activation\n",
    "- **Output**: 10 neurons (one per digit 0-9)\n",
    "\n",
    "---\n",
    "\n",
    "## Demo: Building a Simple Network\n",
    "\n",
    "The instructor will demonstrate:\n",
    "- How to subclass nn.Module\n",
    "- Defining layers in `__init__`\n",
    "- Implementing the `forward()` method\n",
    "- Moving the model to GPU/CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo: Building a Simple MNIST Classifier\n\nclass SimpleMNISTNet(nn.Module):\n    def __init__(self):\n        super().__init__()  # MUST call parent constructor\n        \n        # Define layers as attributes (registered automatically)\n        self.fc1 = nn.Linear(784, 128)  # Input: 784 pixels, Output: 128 features\n        self.fc2 = nn.Linear(128, 10)   # Input: 128 features, Output: 10 classes\n    \n    def forward(self, x):\n        # Define forward pass computation\n        x = x.view(x.size(0), -1)  # Flatten: (batch, 1, 28, 28) -> (batch, 784)\n        x = torch.relu(self.fc1(x))  # Apply first layer + ReLU activation\n        x = self.fc2(x)  # Apply second layer (no activation - raw logits)\n        return x\n\n# Create model and move to device\nmodel = SimpleMNISTNet().to(device)\nprint(model)\nprint(f\"\\nModel is on: {next(model.parameters()).device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Understanding Model Parameters\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test with dummy data\n",
    "dummy_input = torch.randn(5, 1, 28, 28).to(device) # Batch of 5 images\n",
    "output = model(dummy_input)\n",
    "print(f\"\\nInput shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\") # Should be (5, 10)\n",
    "print(f\"\\n Output has 10 values per image - one score for each digit (0-9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab 3: Build Your Own Neural Network\n",
    "\n",
    "**Task**: Create a deeper network with architecture **784 → 256 → 128 → 10**\n",
    "\n",
    "Requirements:\n",
    "1. Create a class called `DeepMNISTNet` that subclasses `nn.Module`\n",
    "2. Define three linear layers:\n",
    "- `fc1`: 784 → 256\n",
    "- `fc2`: 256 → 128\n",
    "- `fc3`: 128 → 10\n",
    "3. In the forward pass:\n",
    "- Flatten the input\n",
    "- Apply fc1, then ReLU\n",
    "- Apply fc2, then ReLU\n",
    "- Apply fc3 (no activation)\n",
    "4. Create an instance and test it with dummy data\n",
    "\n",
    "**Hints:**\n",
    "- Follow the pattern from `SimpleMNISTNet`\n",
    "- Use `torch.relu()` for activations\n",
    "- Don't forget to call `super().__init__()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Lab 3 - Build Your Own Neural Network\n\nclass DeepMNISTNet(nn.Module):\n    def __init__(self):\n        super().__init__()  # MUST call parent constructor\n        \n        # Define three linear layers: 784 → 256 → 128 → 10\n        self.fc1 = nn.Linear(784, 256)  # Input to first hidden layer\n        self.fc2 = nn.Linear(256, 128)  # First to second hidden layer\n        self.fc3 = nn.Linear(128, 10)   # Second hidden layer to output\n    \n    def forward(self, x):\n        # Flatten input: (batch, 1, 28, 28) -> (batch, 784)\n        x = x.view(x.size(0), -1)\n        \n        # First layer + ReLU activation\n        x = torch.relu(self.fc1(x))\n        \n        # Second layer + ReLU activation\n        x = torch.relu(self.fc2(x))\n        \n        # Output layer (no activation - raw logits for CrossEntropyLoss)\n        x = self.fc3(x)\n        \n        return x\n\n# Test your model\ndeep_model = DeepMNISTNet().to(device)\ntest_input = torch.randn(3, 1, 28, 28).to(device)\ntest_output = deep_model(test_input)\n\nprint(f\"Input shape: {test_input.shape}\")\nprint(f\"Output shape: {test_output.shape}\")  # Should be (3, 10)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in deep_model.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\nprint(\" Model built successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Topic 4: Complete Training Loop\n",
    "\n",
    "## Components of Training\n",
    "\n",
    "To train a neural network, we need:\n",
    "\n",
    "1. **DataLoader**: Automatically batches data and shuffles it\n",
    "2. **Loss Function**: Measures how wrong our predictions are\n",
    "3. **Optimizer**: Updates weights to minimize loss\n",
    "4. **Training Loop**: Repeats the process for multiple epochs\n",
    "\n",
    "### The Training Loop Pattern\n",
    "\n",
    "**CRITICAL PATTERN** - You'll use this for every neural network:\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "for images, labels in train_loader:\n",
    "optimizer.zero_grad() # 1. Zero gradients\n",
    "outputs = model(images) # 2. Forward pass\n",
    "loss = criterion(outputs, labels) # 3. Compute loss\n",
    "loss.backward() # 4. Backward pass (compute gradients)\n",
    "optimizer.step() # 5. Update weights\n",
    "```\n",
    "\n",
    "### For MNIST:\n",
    "\n",
    "- **Loss Function**: `nn.CrossEntropyLoss()` (includes softmax, perfect for classification)\n",
    "- **Optimizer**: `Adam` (adaptive learning rate, works great for beginners)\n",
    "- **Batch Size**: 64 (standard for MNIST)\n",
    "- **Learning Rate**: 0.001 (good default for Adam)\n",
    "\n",
    "---\n",
    "\n",
    "## Demo: Full Training Loop\n",
    "\n",
    "The instructor will demonstrate:\n",
    "- Setting up DataLoaders\n",
    "- Creating loss function and optimizer\n",
    "- The complete training loop\n",
    "- Tracking training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo: Preparing MNIST Data\n\n# Define transforms (convert to tensor and normalize)\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n])\n\n# Load training and test datasets\ntrain_dataset = torchvision.datasets.MNIST(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transform\n)\n\ntest_dataset = torchvision.datasets.MNIST(\n    root='./data',\n    train=False,\n    download=True,\n    transform=transform\n)\n\n# Create DataLoaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Test batches: {len(test_loader)}\")\n\n# Visualize a batch\nimages, labels = next(iter(train_loader))\nprint(f\"\\nBatch shape: {images.shape}\")  # (64, 1, 28, 28)\nprint(f\"Labels shape: {labels.shape}\")  # (64,)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo: Full Training Loop for MNIST\n\n# 1. Create model, loss, optimizer\nmodel = SimpleMNISTNet().to(device)\ncriterion = nn.CrossEntropyLoss()  # Includes softmax + negative log likelihood\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# 2. Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()  # Set to training mode\n    running_loss = 0.0\n    \n    for batch_idx, (images, labels) in enumerate(train_loader):\n        # Move data to device\n        images, labels = images.to(device), labels.to(device)\n        \n        # Training step pattern: zero → forward → loss → backward → step\n        optimizer.zero_grad()        # 1. Zero gradients\n        outputs = model(images)      # 2. Forward pass\n        loss = criterion(outputs, labels)  # 3. Compute loss\n        loss.backward()              # 4. Backward pass (compute gradients)\n        optimizer.step()             # 5. Update weights\n        \n        running_loss += loss.item()\n        \n        # Print progress every 200 batches\n        if batch_idx % 200 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n    \n    avg_loss = running_loss / len(train_loader)\n    print(f'Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}\\n')\n\nprint(\" Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab 4: Train Your Deep Network\n",
    "\n",
    "**Tasks**:\n",
    "1. Train your `DeepMNISTNet` for 5 epochs\n",
    "2. Track loss values in a list and plot the training curve\n",
    "3. Save the trained model to `'mnist_model.pth'`\n",
    "\n",
    "**Requirements**:\n",
    "- Use the same DataLoaders we created above\n",
    "- Use `CrossEntropyLoss` and `Adam` optimizer (lr=0.001)\n",
    "- Follow the training loop pattern: `zero_grad → forward → loss → backward → step`\n",
    "- Store loss values and plot them\n",
    "\n",
    "**Hints:**\n",
    "- Create empty list: `loss_history = []`\n",
    "- Append losses: `loss_history.append(avg_loss)`\n",
    "- Plot: `plt.plot(loss_history)`\n",
    "- Save model: `torch.save(model.state_dict(), 'mnist_model.pth')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Lab 4 - Train Your Deep Network\n\n# 1. Create model, loss, optimizer\ndeep_model = DeepMNISTNet().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(deep_model.parameters(), lr=0.001)\n\n# 2. Training loop\nloss_history = []\nnum_epochs = 5\n\nfor epoch in range(num_epochs):\n    deep_model.train()  # Set to training mode\n    running_loss = 0.0\n    \n    for batch_idx, (images, labels) in enumerate(train_loader):\n        # Move data to device\n        images, labels = images.to(device), labels.to(device)\n        \n        # Training step pattern: zero_grad → forward → loss → backward → step\n        optimizer.zero_grad()           # 1. Zero gradients\n        outputs = deep_model(images)    # 2. Forward pass\n        loss = criterion(outputs, labels)  # 3. Compute loss\n        loss.backward()                 # 4. Backward pass (compute gradients)\n        optimizer.step()                # 5. Update weights\n        \n        running_loss += loss.item()\n    \n    # Calculate average loss for this epoch\n    avg_loss = running_loss / len(train_loader)\n    loss_history.append(avg_loss)\n    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n\nprint(\"\\n Training complete!\")\n\n# 3. Plot training loss\nplt.plot(loss_history, marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Average Loss')\nplt.title('Training Loss Over Time')\nplt.grid(True)\nplt.show()\n\n# 4. Save model\ntorch.save(deep_model.state_dict(), 'mnist_model.pth')\nprint(\"\\n Model saved to 'mnist_model.pth'!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Topic 5: Evaluation & Analysis\n",
    "\n",
    "## Evaluating Neural Networks\n",
    "\n",
    "After training, we need to:\n",
    "1. **Switch to evaluation mode**: `model.eval()` (disables dropout, batch norm tracking)\n",
    "2. **Disable gradient computation**: `torch.no_grad()` (saves memory)\n",
    "3. **Compute accuracy** on test data\n",
    "4. **Analyze where the model fails** (confusion matrix)\n",
    "\n",
    "### Why model.eval() and torch.no_grad()?\n",
    "\n",
    "- `model.eval()`: Changes model behavior (e.g., disables dropout layers)\n",
    "- `torch.no_grad()`: Don't compute gradients (we're not training, saves memory)\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Shows which digits are confused with which:\n",
    "- Diagonal: Correct predictions\n",
    "- Off-diagonal: Mistakes (e.g., predicted 4 when true label was 9)\n",
    "\n",
    "---\n",
    "\n",
    "## Demo: Model Evaluation\n",
    "\n",
    "The instructor will demonstrate:\n",
    "- Computing test accuracy\n",
    "- Creating confusion matrix\n",
    "- Visualizing predictions and mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo: Evaluating the Model\n\nmodel.eval()  # Set to evaluation mode (disables dropout, batch norm tracking)\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():  # Don't compute gradients during evaluation\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)  # Get class with highest score\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = 100 * correct / total\nprint(f'Test Accuracy: {accuracy:.2f}%')\nprint(f'\\n Goal was ~95% accuracy. How did we do?')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo: Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Collect all predictions\nall_preds = []\nall_labels = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.numpy())\n\n# Compute confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\n\n# Visualize\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(10))\ndisp.plot(cmap='Blues', values_format='d')\nplt.title('MNIST Confusion Matrix')\nplt.show()\n\n# Per-class accuracy\nprint(\"\\nPer-digit accuracy:\")\nclass_correct = cm.diagonal()\nclass_total = cm.sum(axis=1)\nfor digit in range(10):\n    acc = 100 * class_correct[digit] / class_total[digit]\n    print(f'Digit {digit}: {acc:.1f}% ({class_correct[digit]}/{class_total[digit]})')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo: Visualize Predictions (Correct and Incorrect)\n\n# Get one batch\nimages, labels = next(iter(test_loader))\nimages_on_device = images.to(device)\n\n# Get predictions\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(images_on_device)\n    _, predicted = torch.max(outputs, 1)\n\n# Move back to CPU for visualization\npredicted = predicted.cpu()\n\n# Show first 10 examples\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(images[i].squeeze(), cmap='gray')\n    true_label = labels[i].item()\n    pred_label = predicted[i].item()\n    color = 'green' if true_label == pred_label else 'red'\n    ax.set_title(f'True: {true_label}, Pred: {pred_label}', color=color)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n Green = correct prediction, Red = incorrect prediction\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lab 5: Analyze Your Model\n",
    "\n",
    "**Tasks**:\n",
    "1. Evaluate your `DeepMNISTNet` on test data and compute accuracy\n",
    "2. Generate a confusion matrix\n",
    "3. Find and visualize 10 misclassified examples\n",
    "4. Analyze which digit pairs are most confused (e.g., 4 vs 9, 5 vs 3)\n",
    "\n",
    "**Requirements**:\n",
    "- Use `model.eval()` and `torch.no_grad()`\n",
    "- Collect all predictions and labels\n",
    "- Use sklearn's `confusion_matrix`\n",
    "- Identify where predicted != true\n",
    "\n",
    "**Questions to answer**:\n",
    "- What's your model's accuracy?\n",
    "- Which digit is hardest to classify?\n",
    "- Which pairs of digits are most confused?\n",
    "- Can you identify patterns in the mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Lab 5 - Analyze Your Model\n\n# 1. Compute test accuracy\ndeep_model.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = deep_model(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = 100 * correct / total\nprint(f\"Test Accuracy: {accuracy:.2f}%\\n\")\n\n# 2. Generate confusion matrix\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nall_preds = []\nall_labels = []\n\ndeep_model.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        outputs = deep_model(images)\n        _, predicted = torch.max(outputs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.numpy())\n\ncm = confusion_matrix(all_labels, all_preds)\ndisp = ConfusionMatrixDisplay(cm, display_labels=range(10))\ndisp.plot(cmap='Blues', values_format='d')\nplt.title('Deep MNIST Model - Confusion Matrix')\nplt.show()\n\n# 3. Find misclassified examples\nmisclassified_images = []\nmisclassified_true = []\nmisclassified_pred = []\n\ndeep_model.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images_on_device = images.to(device)\n        outputs = deep_model(images_on_device)\n        _, predicted = torch.max(outputs, 1)\n        predicted = predicted.cpu()\n        \n        # Find misclassified in this batch\n        for i in range(len(labels)):\n            if predicted[i] != labels[i]:\n                misclassified_images.append(images[i])\n                misclassified_true.append(labels[i].item())\n                misclassified_pred.append(predicted[i].item())\n                \n                if len(misclassified_images) >= 10:\n                    break\n        \n        if len(misclassified_images) >= 10:\n            break\n\n# 4. Visualize misclassified examples\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(misclassified_images[i].squeeze(), cmap='gray')\n    ax.set_title(f'True: {misclassified_true[i]}, Pred: {misclassified_pred[i]}', color='red')\n    ax.axis('off')\nplt.suptitle('Misclassified Examples', fontsize=16, fontweight='bold')\nplt.show()\n\n# 5. Analyze confusion - which digits are most commonly confused?\nprint(\"\\nMost Common Confusions:\")\n# Look at off-diagonal elements\nfor i in range(10):\n    for j in range(10):\n        if i != j and cm[i, j] > 20:  # Show confusions with >20 examples\n            print(f\"  Digit {i} misclassified as {j}: {cm[i, j]} times\")\n\n# Per-digit accuracy\nprint(\"\\nPer-digit accuracy:\")\nclass_correct = cm.diagonal()\nclass_total = cm.sum(axis=1)\nfor digit in range(10):\n    acc = 100 * class_correct[digit] / class_total[digit]\n    print(f'Digit {digit}: {acc:.1f}% ({class_correct[digit]}/{class_total[digit]})')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Optional Labs (Complete at Home)\n",
    "\n",
    "These labs are for students who finish early or want extra practice. They explore different aspects of PyTorch and neural networks.\n",
    "\n",
    "## Optional Lab A: Iris Dataset Classification\n",
    "\n",
    "The Iris dataset is a classic classification problem with only 4 features and 3 classes. Build a simple network to classify iris flowers.\n",
    "\n",
    "**Challenge**: Achieve >95% accuracy on this smaller dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Load the Iris dataset from sklearn\n",
    "2. Split into train/test and normalize features\n",
    "3. Convert to PyTorch tensors\n",
    "4. Build a network: 4 inputs → hidden layer(s) → 3 outputs\n",
    "5. Train and evaluate\n",
    "\n",
    "**Hints**:\n",
    "- Use `sklearn.datasets.load_iris()`\n",
    "- Network can be simpler than MNIST (e.g., 4 → 10 → 3)\n",
    "- Fewer epochs needed (dataset is small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Optional Lab A - Iris Classification\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Split and scale\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Convert to tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\nprint(f\"Training samples: {len(X_train_tensor)}\")\nprint(f\"Test samples: {len(X_test_tensor)}\")\nprint(f\"Features: {X_train_tensor.shape[1]}\")\nprint(f\"Classes: {len(set(y.tolist()))}\")\n\n# 1. Build network: 4 inputs -> hidden layer(s) -> 3 outputs\nclass IrisNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(4, 16)   # Input to hidden layer\n        self.fc2 = nn.Linear(16, 8)   # First to second hidden layer\n        self.fc3 = nn.Linear(8, 3)    # Hidden to output layer\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)  # No activation for logits\n        return x\n\n# Create model\niris_model = IrisNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(iris_model.parameters(), lr=0.01)\n\n# 2. Train the network\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass\n    outputs = iris_model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n    \n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 20 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# 3. Evaluate accuracy\niris_model.eval()\nwith torch.no_grad():\n    outputs = iris_model(X_test_tensor)\n    _, predicted = torch.max(outputs, 1)\n    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n    print(f'\\nTest Accuracy: {accuracy * 100:.2f}%')\n    \n    if accuracy > 0.95:\n        print(\" Goal achieved: >95% accuracy!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Congratulations!\n",
    "\n",
    "You've completed Week 1: PyTorch Basics!\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "PyTorch tensors and operations\n",
    "Automatic differentiation with autograd\n",
    "Building neural networks with nn.Module\n",
    "Complete training loops with proper patterns\n",
    "Model evaluation and analysis\n",
    "Real-world application: Handwritten digit recognition\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Always call `optimizer.zero_grad()`** before computing new gradients\n",
    "2. **Training loop pattern**: `zero_grad → forward → loss → backward → step`\n",
    "3. **Use `.to(device)`** to handle CPU/GPU automatically\n",
    "4. **`model.eval()` and `torch.no_grad()`** for evaluation\n",
    "5. **Confusion matrix** helps identify where your model struggles\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Week 2**: CNNs & RNNs - Transfer learning and sequence modeling\n",
    "- **Practice**: Complete the optional Iris lab\n",
    "- **Challenge**: Try to improve your MNIST model to >97% accuracy\n",
    "- Hint: Try adding more layers, dropout, or data augmentation\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [PyTorch Official Tutorials](https://pytorch.org/tutorials/)\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
    "- [MNIST Dataset Info](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "---\n",
    "\n",
    "**Great work! See you next week for CNNs and RNNs! **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}